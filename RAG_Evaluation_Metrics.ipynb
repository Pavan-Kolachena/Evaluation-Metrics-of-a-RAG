{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Faithfullness"
      ],
      "metadata": {
        "id": "1yswQImePL8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Simulate retrieved documents (in practice, these would be retrieved from your retrieval system)\n",
        "retrieved_docs = [\n",
        "    \"The sky is blue and the sun is shining.\",\n",
        "    \"Clouds can form when the atmosphere cools and water vapor condenses.\",\n",
        "    \"A clear sky indicates good weather.\"\n",
        "]\n",
        "\n",
        "# Simulate a generated response (in practice, this would be generated by your language model)\n",
        "generated_response = \"The sky is clear and blue, and there are no clouds.\"\n",
        "\n",
        "# Combine the retrieved documents into a single string\n",
        "retrieved_text = \" \".join(retrieved_docs)\n",
        "\n",
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer().fit_transform([retrieved_text, generated_response])\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "# Calculate cosine similarity between the retrieved text and the generated response\n",
        "cosine_sim = cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
        "\n",
        "# Define a simple threshold for faithfulness (e.g., 0.5 is arbitrary here, adjust as needed)\n",
        "faithfulness_threshold = 0.5\n",
        "\n",
        "# Determine faithfulness\n",
        "if cosine_sim >= faithfulness_threshold:\n",
        "    print(f\"The generated response is faithful (Cosine Similarity: {cosine_sim:.2f})\")\n",
        "else:\n",
        "    print(f\"The generated response is not faithful (Cosine Similarity: {cosine_sim:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7z7hvw7PQfQ",
        "outputId": "0677344e-af28-4ea3-f568-01dd1e92ff04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated response is not faithful (Cosine Similarity: 0.50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Relavency"
      ],
      "metadata": {
        "id": "06qN6cViRAtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Simulate a query\n",
        "query = \"What causes the sky to appear blue?\"\n",
        "\n",
        "# Simulate a generated answer from your RAG pipeline\n",
        "generated_answer = \"The sky appears blue due to the scattering of sunlight by the atmosphere.\"\n",
        "\n",
        "# Vectorize the query and the generated answer using TF-IDF\n",
        "vectorizer = TfidfVectorizer().fit_transform([query, generated_answer])\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "# Calculate cosine similarity between the query and the generated answer\n",
        "cosine_sim = cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
        "\n",
        "# Define a simple threshold for relevance (e.g., 0.5 is arbitrary here, adjust as needed)\n",
        "relevance_threshold = 0.5\n",
        "\n",
        "# Determine relevance\n",
        "if cosine_sim >= relevance_threshold:\n",
        "    print(f\"The generated answer is relevant (Cosine Similarity: {cosine_sim:.2f})\")\n",
        "else:\n",
        "    print(f\"The generated answer is not relevant (Cosine Similarity: {cosine_sim:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2TEPXPNREXH",
        "outputId": "ff6733de-ec2e-4183-cb67-0e9396bdf461"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated answer is not relevant (Cosine Similarity: 0.37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context Precision"
      ],
      "metadata": {
        "id": "QZM7QG9MZG7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Simulate a query\n",
        "query = \"What causes the sky to appear blue?\"\n",
        "\n",
        "# Simulate retrieved contexts from your RAG pipeline\n",
        "retrieved_contexts = [\n",
        "    \"The sky appears blue due to the scattering of sunlight by the Earth's atmosphere.\",\n",
        "    \"Rainbows are formed when light is refracted through water droplets.\",\n",
        "    \"The color of the sky is affected by the scattering of light by air molecules.\"\n",
        "]\n",
        "\n",
        "# Combine the query and each retrieved context into a list\n",
        "texts = [query] + retrieved_contexts\n",
        "\n",
        "# Vectorize the texts using TF-IDF\n",
        "vectorizer = TfidfVectorizer().fit_transform(texts)\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "# Calculate cosine similarity between the query and each retrieved context\n",
        "similarities = cosine_similarity([vectors[0]], vectors[1:])[0]\n",
        "\n",
        "# Define a simple threshold for context relevance (e.g., 0.5 is arbitrary here, adjust as needed)\n",
        "relevance_threshold = 0.5\n",
        "\n",
        "# Determine the number of relevant contexts\n",
        "relevant_contexts_count = sum(sim >= relevance_threshold for sim in similarities)\n",
        "\n",
        "# Calculate precision (ratio of relevant contexts to total retrieved contexts)\n",
        "context_precision = relevant_contexts_count / len(retrieved_contexts)\n",
        "\n",
        "print(f\"Context Precision: {context_precision:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo6_fml_ZKwW",
        "outputId": "3df25454-c8b1-42ce-ccac-79201aaf228c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Precision: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context Relavency"
      ],
      "metadata": {
        "id": "BIKll1O_ZXyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Simulate a query\n",
        "query = \"What causes the sky to appear blue?\"\n",
        "# Simulate retrieved contexts from your RAG pipeline\n",
        "retrieved_contexts = [\n",
        "    \"The sky appears blue due to the scattering of sunlight by the Earth's atmosphere.\",\n",
        "    \"Rainbows are formed when light is refracted through water droplets.\",\n",
        "    \"The color of the sky is affected by the scattering of light by air molecules.\"\n",
        "]\n",
        "# Combine the query and each retrieved context into a list\n",
        "texts = [query] + retrieved_contexts\n",
        "# Vectorize the texts using TF-IDF\n",
        "vectorizer = TfidfVectorizer().fit_transform(texts)\n",
        "vectors = vectorizer.toarray()\n",
        "# Calculate cosine similarity between the query and each retrieved context\n",
        "similarities = cosine_similarity([vectors[0]], vectors[1:])[0]\n",
        "# Define a simple threshold for context relevancy (e.g., 0.5 is arbitrary here, adjust as needed)\n",
        "relevancy_threshold = 0.5\n",
        "# Determine relevancy of each context\n",
        "relevancy_results = [\n",
        "    f\"Context {i+1} is {'relevant' if sim >= relevancy_threshold else 'not relevant'} (Cosine Similarity: {sim:.2f})\"\n",
        "    for i, sim in enumerate(similarities)\n",
        "]\n",
        "# Print the relevancy results for each context\n",
        "for result in relevancy_results:\n",
        "  print(result)\n",
        "# Calculate overall context relevancy (e.g., average similarity or percentage of relevant contexts)\n",
        "average_similarity = np.mean(similarities)\n",
        "relevant_contexts_count = sum(sim >= relevancy_threshold for sim in similarities)\n",
        "print(f\"\\nAverage Context Relevancy: {average_similarity:.2f}\")\n",
        "print(f\"Number of Relevant Contexts: {relevant_contexts_count} out of {len(retrieved_contexts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQZXBD7eZmyC",
        "outputId": "7db4d8c3-cb8d-48bc-a298-e0372a0fd909"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1 is not relevant (Cosine Similarity: 0.37)\n",
            "Context 2 is not relevant (Cosine Similarity: 0.00)\n",
            "Context 3 is not relevant (Cosine Similarity: 0.19)\n",
            "\n",
            "Average Context Relevancy: 0.18\n",
            "Number of Relevant Contexts: 0 out of 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Context recall\n"
      ],
      "metadata": {
        "id": "bT3xvCz5ci62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Simulate a query\n",
        "query = \"What causes the sky to appear blue?\"\n",
        "\n",
        "# Simulate retrieved contexts from your RAG pipeline\n",
        "retrieved_contexts = [\n",
        "    \"The sky appears blue due to the scattering of sunlight by the Earth's atmosphere.\",\n",
        "    \"Rainbows are formed when light is refracted through water droplets.\",\n",
        "    \"The color of the sky is affected by the scattering of light by air molecules.\"\n",
        "]\n",
        "\n",
        "# Simulate a list of all possible relevant contexts (for example, from a ground truth or gold standard)\n",
        "all_relevant_contexts = [\n",
        "    \"The sky appears blue due to the scattering of sunlight by the Earth's atmosphere.\",\n",
        "    \"The color of the sky is affected by the scattering of light by air molecules.\",\n",
        "    \"The sky appears blue because blue light is scattered more than other colors due to Rayleigh scattering.\"\n",
        "]\n",
        "\n",
        "# Combine the query and all relevant contexts into a list for vectorization\n",
        "texts = [query] + retrieved_contexts + all_relevant_contexts\n",
        "\n",
        "# Vectorize the texts using TF-IDF\n",
        "vectorizer = TfidfVectorizer().fit_transform(texts)\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "# Calculate cosine similarity between the query and each retrieved context\n",
        "retrieved_similarities = cosine_similarity([vectors[0]], vectors[1:len(retrieved_contexts)+1])[0]\n",
        "\n",
        "# Calculate cosine similarity between the query and each relevant context\n",
        "all_relevant_similarities = cosine_similarity([vectors[0]], vectors[len(retrieved_contexts)+1:])[0]\n",
        "\n",
        "# Define a simple threshold for relevance (e.g., 0.5 is arbitrary here, adjust as needed)\n",
        "relevance_threshold = 0.5\n",
        "\n",
        "# Count relevant retrieved contexts\n",
        "relevant_retrieved_count = sum(sim >= relevance_threshold for sim in retrieved_similarities)\n",
        "\n",
        "# Count total relevant contexts\n",
        "total_relevant_count = sum(sim >= relevance_threshold for sim in all_relevant_similarities)\n",
        "\n",
        "# Calculate recall (ratio of relevant retrieved contexts to total relevant contexts)\n",
        "context_recall = relevant_retrieved_count / total_relevant_count if total_relevant_count > 0 else 0\n",
        "\n",
        "print(f\"Context Recall: {context_recall:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDx2OxAfgHiU",
        "outputId": "fb4fd837-36a5-44ab-87dd-93ed6c85cafc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Recall: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Context Entities Recall"
      ],
      "metadata": {
        "id": "ldcAAPm-gX1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model for entity recognition (you can use \"en_core_web_sm\" or a similar model)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Simulate a query\n",
        "query = \"What causes the sky to appear blue?\"\n",
        "\n",
        "\n",
        "\n",
        "# Simulate retrieved contexts from your RAG pipeline\n",
        "retrieved_contexts = [\n",
        "    \"The sky appears blue due to the scattering of sunlight by the Earth's atmosphere.\",\n",
        "    \"Rainbows are formed when light is refracted through water droplets.\",\n",
        "    \"The color of the sky is affected by the scattering of light by air molecules.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Extract entities from the query\n",
        "query_doc = nlp(query)\n",
        "query_entities = {ent.text.lower() for ent in query_doc.ents}\n",
        "\n",
        "# Extract entities from each retrieved context\n",
        "retrieved_entities = set()\n",
        "for context in retrieved_contexts:\n",
        "  context_doc = nlp(context)\n",
        "  retrieved_entities.update({ent.text.lower() for ent in context_doc.ents})\n",
        "\n",
        "# Calculate recall: the ratio of relevant entities retrieved\n",
        "if query_entities:\n",
        "    entities_recall = len(query_entities & retrieved_entities) / len(query_entities)\n",
        "else:\n",
        "    entities_recall = 0.0\n",
        "\n",
        "print(f\"Context Entities Recall: {entities_recall:.2f}\")\n",
        "\n",
        "# Optional: Print entities for verification\n",
        "print(f\"Query Entities: {query_entities}\")\n",
        "print(f\"Retrieved Entities: {retrieved_entities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTrk28yhgahB",
        "outputId": "9a87437e-8130-41a9-b3bd-d6d1cbad0b05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Entities Recall: 0.00\n",
            "Query Entities: set()\n",
            "Retrieved Entities: {'earth'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Semantic Similarity"
      ],
      "metadata": {
        "id": "IKyk4UYShXMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Simulate a reference answer (ground truth)\n",
        "reference_answer = \"The sky appears blue due to the scattering of sunlight by the Earth's atmosphere.\"\n",
        "\n",
        "# Simulate an answer generated by the RAG pipeline\n",
        "generated_answer = \"The reason the sky looks blue is because of the scattering of sunlight by the atmosphere.\"\n",
        "\n",
        "# Combine the reference answer and generated answer into a list\n",
        "texts = [reference_answer, generated_answer]\n",
        "\n",
        "# Vectorize the texts using TF-IDF\n",
        "vectorizer = TfidfVectorizer().fit_transform(texts)\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "# Calculate cosine similarity between the reference answer and the generated answer\n",
        "semantic_similarity = cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
        "\n",
        "# Print the semantic similarity score\n",
        "print(f\"Answer Semantic Similarity: {semantic_similarity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etjKzKTHhZ-L",
        "outputId": "447499b0-4402-4825-8742-3b3ff9c6b76f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer Semantic Similarity: 0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Correctness"
      ],
      "metadata": {
        "id": "FWZL6eolhz-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Simulate a reference answer (ground truth)\n",
        "reference_answer = \"The sky appears blue due to the scattering of sunlight by the Earth's atmosphere.\"\n",
        "\n",
        "# Simulate an answer generated by the RAG pipeline\n",
        "generated_answer = \"The sky is blue because of the scattering of sunlight by the atmosphere.\"\n",
        "\n",
        "# Convert answers to lowercase for case-insensitive comparison\n",
        "reference_answer = reference_answer.lower()\n",
        "generated_answer = generated_answer.lower()\n",
        "\n",
        "# Method 1: Exact Match\n",
        "exact_match = reference_answer == generated_answer\n",
        "\n",
        "# Method 2: Keyword Overlap\n",
        "# Define a simple function to extract key terms\n",
        "def extract_keywords(text):\n",
        "    vectorizer = CountVectorizer().fit([text])\n",
        "    return set(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Extract keywords from both reference and generated answers\n",
        "reference_keywords = extract_keywords(reference_answer)\n",
        "generated_keywords = extract_keywords(generated_answer)\n",
        "\n",
        "# Calculate the overlap\n",
        "keyword_overlap = len(reference_keywords & generated_keywords) / len(reference_keywords)\n",
        "\n",
        "# Define a threshold for correctness based on keyword overlap (e.g., 0.8 for 80% overlap)\n",
        "correctness_threshold = 0.8\n",
        "is_correct = keyword_overlap >= correctness_threshold\n",
        "\n",
        "# Print the results\n",
        "print(f\"Exact Match Correctness: {'Correct' if exact_match else 'Incorrect'}\")\n",
        "print(f\"Keyword Overlap: {keyword_overlap:.2f}\")\n",
        "print(f\"Answer Correctness: {'Correct' if is_correct else 'Incorrect'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDtF2aaoh19O",
        "outputId": "0af83920-3d34-4de4-e362-639930e8b9fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Correctness: Incorrect\n",
            "Keyword Overlap: 0.67\n",
            "Answer Correctness: Incorrect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect Critique"
      ],
      "metadata": {
        "id": "qp8d6dUFiI8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Simulate a reference answer (ground truth) with identified aspects\n",
        "reference_answer = {\n",
        "    \"aspect_1\": \"The sky appears blue.\",\n",
        "    \"aspect_2\": \"This is due to the scattering of sunlight.\",\n",
        "    \"aspect_3\": \"The Earth's atmosphere causes this scattering.\"\n",
        "}\n",
        "\n",
        "# Simulate an answer generated by the RAG pipeline\n",
        "generated_answer = \"The sky looks blue because sunlight is scattered by the atmosphere.\"\n",
        "\n",
        "# Function to evaluate if each aspect is covered in the generated answer\n",
        "def evaluate_aspect_coverage(reference_aspects, generated_answer):\n",
        "    # Vectorize the aspects and generated answer using TF-IDF\n",
        "    vectorizer = TfidfVectorizer().fit_transform(list(reference_aspects.values()) + [generated_answer])\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    aspect_coverage = {}\n",
        "\n",
        "    for i, (aspect, text) in enumerate(reference_aspects.items()):\n",
        "        # Calculate cosine similarity between the aspect and the generated answer\n",
        "        similarity = cosine_similarity([vectors[i]], [vectors[-1]])[0][0]\n",
        "        # Define a threshold for considering the aspect sufficiently covered\n",
        "        coverage_threshold = 0.5\n",
        "        aspect_coverage[aspect] = similarity >= coverage_threshold\n",
        "\n",
        "    return aspect_coverage\n",
        "\n",
        "# Evaluate aspect coverage\n",
        "aspect_coverage = evaluate_aspect_coverage(reference_answer, generated_answer)\n",
        "\n",
        "# Print the aspect critique results\n",
        "for aspect, covered in aspect_coverage.items():\n",
        "    print(f\"{aspect}: {'Covered' if covered else 'Not Covered'}\")\n",
        "\n",
        "# Optional: Calculate overall aspect critique score (percentage of aspects covered)\n",
        "overall_coverage = sum(covered for covered in aspect_coverage.values()) / len(aspect_coverage)\n",
        "print(f\"\\nOverall Aspect Coverage: {overall_coverage * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZJ84nl9iLyp",
        "outputId": "c158dfeb-7072-411a-a81e-c64a560ac844"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aspect_1: Not Covered\n",
            "aspect_2: Not Covered\n",
            "aspect_3: Not Covered\n",
            "\n",
            "Overall Aspect Coverage: 0.00%\n"
          ]
        }
      ]
    }
  ]
}